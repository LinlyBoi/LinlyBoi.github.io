\documentclass[theme=sleek, randomorder, hidesidemenu]{webquiz}
\title{Tokenisation}
\begin{document}
\begin{question}

  Space-based tokenisation can be used for most languages like (Select all that apply)

  \begin{choice}[multiple, columns=2]
    \incorrect Japanese
    \incorrect Chinese
    \correct Arabic
    \incorrect Thai
    \correct Cyrillic
    \correct English
  \end{choice}

\end{question}

\begin{question}

  Space-based tokenisation can be done with unix tools like:

  \begin{choice}[columns=2]
    \incorrect ed \feedback this is a text editor
    \correct tr
    \incorrect cp \feedback this copies...
    \incorrect cut \feedback this removes specific parts of a line based on position
  \end{choice}

\end{question}

\begin{question}

  We can remove punctuation since it holds no real meaning for machines

  \begin{choice}[columns=2]
    \incorrect True
    \correct False
  \end{choice}

\end{question}

\begin{question}

  Byte-Pair Encoding is composed of 2 parts which are

  \begin{choice}[columns=2]
    \incorrect learner and tokeniser
    \correct segmenter and learner
    \incorrect vocab corpus and tokens
    \incorrect segmenter and classifier
  \end{choice}

\end{question}

\begin{question}

  Byte-Pair Encoding often includes subwords like morphemes

  \begin{choice}[columns=2]
    \correct True
    \incorrect False
  \end{choice}

\end{question}

\begin{question}

  Morphological parsing is

  \begin{choice}[columns=2]
    \correct Dividing words into the smallest subwords with meaning
    \incorrect Finding which words have been morphed or modified
    \incorrect Identify changes that occurred to the root word
    \incorrect Parsing own words by morphing them into another entity or token
  \end{choice}

\end{question}

\begin{question}

  Stemming is a more sophisticated form of lemmatization

  \begin{choice}[columns]
    \incorrect True
    \correct False
  \end{choice}

\end{question}

\begin{question}

  Reducing relational to relate via a rule that turns ``ATIONAL'' TO ``ATE'' is an example of

  \begin{choice}[columns=2]
    \incorrect Stemming
    \correct Porter Stemmer
    \incorrect Lemmatisation
    \incorrect Regular Expressions
  \end{choice}

\end{question}

\begin{question}

  The most problematic symbol when segmenting sentences:

  \begin{choice}[columns=2]
    \incorrect ?
    \incorrect !
    \incorrect , \feedback that doesn't even separate sentences\ldots
    \correct .
  \end{choice}
\end{question}

\begin{question}

  What can we use to deal with symbols that may mark an end of sentence or part of a word?

  \begin{choice}
    \correct Use rules or machine learning to classify
    \incorrect Use word segmentation and prune symbols
    \incorrect Use POS (part of speech tagging)
    \incorrect Tokenise after filtering out symbols
  \end{choice}

\end{question}

\begin{question}

  What can we use to mitigate email scams?
  \begin{choice}[columns=2]
    \incorrect Text processing
    \correct Text classification
    \incorrect Sentiment analysis
    \incorrect Lemmatisation
  \end{choice}

\end{question}

\begin{question}

  Hybrid machine translation relies on
  \begin{choice}[columns=2]
    \correct Text classification
    \incorrect Text processing
    \incorrect Recurring Neural Networks
    \incorrect Data filtering
  \end{choice}

\end{question}

\begin{question}

  The first layer of Text classification is
  \begin{choice}[columns=2]
    \incorrect Classification Algorithm
    \correct Feature extract
    \incorrect Vector of features
    \incorrect Class label
    \incorrect Probability distribution
  \end{choice}

\end{question}

\begin{question}

  It is not possible to generate classification features by hand

  \begin{choice}
    \incorrect True
    \correct False
  \end{choice}

\end{question}

\begin{question}
  Which of the following is not true about the BOW representation for text? (Bag of words)

  \begin{choice}
  \incorrect Represented by vectors or matrices of 1s and 0s
  \incorrect Each word represented equally
  \correct Context taken into consideration
  \incorrect Features is the number of unique words in vocabulary
  \end{choice}

\end{question}

\begin{question}

  What solves the space dimensionality problem of BOW?

  \begin{choice}[columns=2, multiple]
    \correct NBOW
    \incorrect Using Ngrams
    \incorrect Linearising sum
    \correct Using pre-trained word embeddings \feedback NBOW is correct too :)
  \end{choice}

\end{question}

\begin{question}

  NBOW retains the problem of the Bag of Words where all words are treated equally

  \begin{choice}[columns=2]
    \incorrect True
    \correct False
  \end{choice}

\end{question}

\begin{question}
Given the data below
\begin{table}[]
\begin{tabular}{llll}
 & Doc & Sentence                           & Class \\
 & 1   & Captain Price  cool guy            & COD   \\
 & 2   & Ana, Winston  captain?             & OW    \\
 & 3   & Bravo six dark                       & COD   \\
 & 4   & Winston years ago                  & OW    \\
 & 5   & Ana Winston Winston                & OW
\end{tabular}
\end{table}
Find the probability for the classification of the following sentence ``Winston Ana dark'' for it being the class OW (2 non zero decimal places)
\answer[number]{0.0015}
\end{question}


\begin{question}
Given the data below
\begin{table}[]
\begin{tabular}{llll}
 & Doc & Sentence                           & Class \\
 & 1   & Captain Price  cool guy            & COD   \\
 & 2   & Ana, Winston  captain?             & OW    \\
 & 3   & Bravo six dark                       & COD   \\
 & 4   & Winston years ago                  & OW    \\
 & 5   & Ana Winston Winston                & OW
\end{tabular}
\end{table}
Find the probability for the classification of the following sentence ``Winston Ana dark'' for it being the class COD (2 non zero decimal places)
\answer[number]{0.00034}
\end{question}

\end{document}
